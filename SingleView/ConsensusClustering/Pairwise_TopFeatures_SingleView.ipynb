{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad2aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from mvlearn.cluster import MultiviewKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "from ConsensusClusteringMultiView import ConsensusCluster\n",
    "import scipy.stats as sps\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7989ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_F_stat(data):\n",
    "    SSB = (\n",
    "        ((data.groupby(\"cluster\").mean() - data.drop(\"cluster\", 1).mean()) ** 2).T\n",
    "        * data.groupby(\"cluster\").size().values\n",
    "    ).sum(1)\n",
    "\n",
    "    SSW = []\n",
    "    for k in sorted(data[\"cluster\"].unique()):\n",
    "        cluster = data[data[\"cluster\"] == k].drop(\"cluster\", 1)\n",
    "        diff = (cluster - data.groupby(\"cluster\").mean().loc[k]) ** 2\n",
    "        SSW.append(diff)\n",
    "    SSW = pd.concat(SSW).sum()\n",
    "    coef = (len(data) - len(data[\"cluster\"].unique())) / (\n",
    "        len(data[\"cluster\"].unique()) - 1\n",
    "    )\n",
    "    F_stat = (coef * (SSB / SSW)).sort_values(ascending=False)\n",
    "    return F_stat\n",
    "\n",
    "def plot_top(data, target_var, save_path, name):\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    target_cluster = np.unique(data[\"cluster\"])\n",
    "    for i, var in enumerate(target_var):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        for k in target_cluster:\n",
    "            cluster = data[data[\"cluster\"] == k]\n",
    "            plt.hist(\n",
    "                cluster[var],\n",
    "                bins=25,\n",
    "                alpha=0.5,\n",
    "                density=True,\n",
    "                histtype=\"stepfilled\",\n",
    "                label=\"cluster {}, size {}\".format(k, len(cluster)),\n",
    "            )\n",
    "        plt.legend()\n",
    "        plt.title(var, fontsize=15)\n",
    "    plt.suptitle(name, fontsize=25)\n",
    "   \n",
    "    plt.savefig(\n",
    "        \"{}/{}.png\".format(save_path, name), dpi=300,\n",
    "    )\n",
    "    plt.close()\n",
    "    \n",
    "def get_pw_F_stat(data, num_var):\n",
    "    F_stat_true = cal_F_stat(data)\n",
    "    all_F_stat = []\n",
    "    n_test = 19 * num_var\n",
    "    \n",
    "    for i in tqdm(range(n_test)):\n",
    "        data_run = data.copy()\n",
    "        rs = np.random.RandomState(i)\n",
    "        data_run[\"cluster\"] = rs.randint(1, len(data['cluster'].unique()) + 1, size=len(data_run))\n",
    "        F_stat = cal_F_stat(data_run)\n",
    "        all_F_stat.append(F_stat)\n",
    "    \n",
    "    all_F_stat = pd.concat(all_F_stat, 1)\n",
    "    all_F_stat.columns = [\"random {}\".format(i + 1) for i in range(all_F_stat.shape[1])]\n",
    "    all_F_stat[\"true\"] = F_stat_true\n",
    "    return all_F_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97c6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "score_path = \"{}/Clustering_silhouette/\".format(data_path)\n",
    "cdf_path = \"{}/CDF plots/\".format(data_path)\n",
    "tsne_path = \"{}/TSNEplots/\".format(data_path)\n",
    "KCC_path = \"{}/KCC/\".format(data_path)\n",
    "f_stat_path = '{}/F_stat/'.format(data_path)\n",
    "top_feature_plot_path = '{}/Top_features/'.format(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccf2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteome_view = pd.read_csv(\n",
    "    \"{}/ProteomeViewStandardized.csv\".format(data_path), index_col=0\n",
    ")\n",
    "contextual_view = pd.read_csv(\n",
    "    \"{}/ContextualViewStandardized.csv\".format(data_path), index_col=0\n",
    ")\n",
    "physio_view = pd.read_csv(\n",
    "    \"{}/PhysioViewStandardized.csv\".format(data_path), index_col=0\n",
    ")\n",
    "clinical_binary = pd.read_csv(\"{}/ClinicalBinary.csv\".format(data_path), index_col=0)\n",
    "npx = pd.read_csv(\"../olinks/20191053_Giannoni_NPX.csv\")\n",
    "npx.set_index(\"Panel\", inplace=True)\n",
    "proteome_view = pd.read_csv(\n",
    "    \"{}/ProteomeViewStandardized.csv\".format(data_path), index_col=0\n",
    ")\n",
    "\n",
    "uniport = {}\n",
    "for k, p in enumerate(npx.columns):\n",
    "    uniport[p] = npx.iloc[:, k][\"Uniprot ID\"]\n",
    "uniprot_col = []\n",
    "for col in proteome_view.columns:\n",
    "    uniprot_col.append(uniport[col])\n",
    "\n",
    "proteome_view.columns = uniprot_col\n",
    "\n",
    "concated_view = pd.concat([physio_view, contextual_view, proteome_view], 1)\n",
    "all_features = pd.concat([concated_view,clinical_binary ], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42f48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_continous_vars = concated_view.shape[1]\n",
    "nb_binary_vars = clinical_binary.shape[1]\n",
    "total_vars = nb_continous_vars + nb_binary_vars \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b286a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    [\"clinical\", 4, \"ConsensusKMeans\"],\n",
    "    [\"contextual\", 5, \"ConsensusKMeans\"],\n",
    "    [\"physio\", 3, \"DBSCAN\"],\n",
    "    [\"proteome\", 3, \"ConsensusKMeans\"],\n",
    "    [\"proteome\", 4, \"ConsensusKMeans\"],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff201e6a",
   "metadata": {},
   "source": [
    "# pair-wise Fisher exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f97d7850",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           assignment\n",
      "sample.id            \n",
      "BE-001              3\n",
      "BE-003              4\n",
      "BE-004              4\n",
      "BE-005              2\n",
      "BE-007              1\n",
      "(387, 83)\n",
      "           assignment\n",
      "sample.id            \n",
      "BE-001              5\n",
      "BE-003              4\n",
      "BE-004              2\n",
      "BE-005              2\n",
      "BE-007              3\n",
      "(387, 83)\n",
      "           assignment\n",
      "sample.id            \n",
      "BE-001              0\n",
      "BE-003              1\n",
      "BE-004              2\n",
      "BE-005              1\n",
      "BE-007              1\n",
      "(368, 83)\n",
      "           assignment\n",
      "sample.id            \n",
      "BE-001              1\n",
      "BE-003              2\n",
      "BE-004              1\n",
      "BE-005              2\n",
      "BE-007              1\n",
      "(387, 83)\n",
      "           assignment\n",
      "sample.id            \n",
      "BE-001              1\n",
      "BE-003              2\n",
      "BE-004              4\n",
      "BE-005              2\n",
      "BE-007              4\n",
      "(387, 83)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(configs)):\n",
    "    view, KCC_space, method = configs[i]\n",
    "\n",
    "    assignment = pd.read_csv(\n",
    "        \"{}/{}_{}_view_KCC_{}_assignments.csv\".format(\n",
    "            score_path, method, view, KCC_space\n",
    "        ),\n",
    "        index_col=0,\n",
    "    )\n",
    "    print(assignment.head())\n",
    "    if method == \"DBSCAN\":\n",
    "        assignment[\"assignment\"] = assignment[\"assignment\"] + 1\n",
    "        assignment = assignment[assignment[\"assignment\"] != 0]\n",
    "    assignment[\"assignment\"] = assignment[\"assignment\"].astype(int)\n",
    "    data = clinical_binary.copy()\n",
    "    data[\"cluster\"] = assignment[\"assignment\"]\n",
    "    data = data[data[\"cluster\"].notnull()]\n",
    "    \n",
    "    num_cluster = len(data[\"cluster\"].unique())\n",
    "    all_clusters = sorted(data[\"cluster\"].unique())\n",
    "\n",
    "    for i in all_clusters:\n",
    "        for j in all_clusters:\n",
    "            if j > i:\n",
    "                i, j = int(i), int(j)\n",
    "                all_p_val = pd.Series(index=clinical_binary.columns)\n",
    "                cluster_pw = data[(data[\"cluster\"] == i) | (data[\"cluster\"] == j)]\n",
    "                save_name = \"{}_{}_KCC_{}_FisherExact_cluster_{}vs{}.csv\".format(\n",
    "                    method, view, KCC_space, i, j\n",
    "                )\n",
    "\n",
    "                for col in clinical_binary.columns:\n",
    "                    try:\n",
    "                        contingency_table = pd.crosstab(\n",
    "                            cluster_pw[col], cluster_pw[\"cluster\"]\n",
    "                        )\n",
    "                        p_val = stats.fisher_exact(contingency_table)[1]\n",
    "                        all_p_val.loc[col] = p_val\n",
    "                    except:\n",
    "                        pass\n",
    "                all_p_val.to_csv(\"{}/{}\".format(f_stat_path, save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46954fe4",
   "metadata": {},
   "source": [
    "# pair-wise F-stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ef23ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10374/10374 [01:19<00:00, 130.60it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:14<00:00, 138.64it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:16<00:00, 136.37it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:12<00:00, 143.04it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.11it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:09<00:00, 149.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 142.06it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:10<00:00, 146.63it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.87it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:12<00:00, 142.59it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:10<00:00, 147.63it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.51it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:12<00:00, 143.13it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:11<00:00, 145.67it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:09<00:00, 148.56it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10374/10374 [01:18<00:00, 132.26it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:15<00:00, 137.73it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 140.94it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:16<00:00, 135.53it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:14<00:00, 139.97it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:11<00:00, 144.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10374/10374 [01:21<00:00, 127.83it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:21<00:00, 127.96it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:17<00:00, 134.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10374/10374 [01:15<00:00, 136.65it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:14<00:00, 139.02it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.60it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:16<00:00, 135.66it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:14<00:00, 138.37it/s]\n",
      "100%|████████████████████████████████████| 10374/10374 [01:13<00:00, 141.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(configs)):\n",
    "    view, KCC_space, method = configs[i]\n",
    "    assignment = pd.read_csv(\n",
    "        \"{}/{}_{}_view_KCC_{}_assignments.csv\".format(\n",
    "            score_path, method, view, KCC_space\n",
    "        ),\n",
    "        index_col=0,\n",
    "    )\n",
    "    \n",
    "    if method == \"DBSCAN\":\n",
    "        assignment[\"assignment\"] = assignment[\"assignment\"] + 1\n",
    "        assignment = assignment[assignment[\"assignment\"] != 0]\n",
    "\n",
    "    data = concated_view.copy()\n",
    "    data[\"cluster\"] = assignment[\"assignment\"]\n",
    "    data = data[data[\"cluster\"].notnull()]\n",
    "    print(data.shape)\n",
    "    num_cluster = len(data[\"cluster\"].unique())\n",
    "    all_clusters = sorted(data[\"cluster\"].unique())\n",
    "\n",
    "    for i in all_clusters:\n",
    "        for j in all_clusters:\n",
    "            if j > i:\n",
    "                i, j = int(i), int(j)\n",
    "                cluster_pw = data[(data[\"cluster\"] == i) | (data[\"cluster\"] == j)]\n",
    "                save_name = \"{}_{}_KCC_{}_F_stat_cluster_{}vs{}.csv\".format(\n",
    "                    method, view, KCC_space, i, j\n",
    "                )\n",
    "\n",
    "                all_F_stat = get_pw_F_stat(cluster_pw, total_vars)\n",
    "                F_stat_pw = all_F_stat[\"true\"]\n",
    "                F_stat_pw_pvalue = (\n",
    "                    all_F_stat.rank(axis=1, ascending=False)[\"true\"]\n",
    "                    / all_F_stat.shape[1]\n",
    "                )\n",
    "                F_stat_pw = pd.concat([F_stat_pw, F_stat_pw_pvalue], 1)\n",
    "                F_stat_pw.columns = [\"F_stat\", \"p-value\"]\n",
    "\n",
    "                F_stat_pw.to_csv(\"{}/{}\".format(f_stat_path, save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9121ec",
   "metadata": {},
   "source": [
    "# plot top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f85f275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 547)\n",
      "(387, 547)\n",
      "(368, 547)\n",
      "(387, 547)\n",
      "(387, 547)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(configs)):\n",
    "    view, KCC_space, method = configs[i]\n",
    "\n",
    "    assignment = pd.read_csv(\n",
    "        \"{}/{}_{}_view_KCC_{}_assignments.csv\".format(\n",
    "            score_path, method, view, KCC_space\n",
    "        ),\n",
    "        index_col=0,\n",
    "    )\n",
    "    if method == \"DBSCAN\":\n",
    "        assignment[\"assignment\"] = assignment[\"assignment\"] + 1\n",
    "        assignment = assignment[assignment[\"assignment\"] != 0]\n",
    "    assignment[\"assignment\"] = assignment[\"assignment\"].astype(int)\n",
    "    data = all_features.copy()\n",
    "    data[\"cluster\"] = assignment[\"assignment\"]\n",
    "    data = data[data[\"cluster\"].notnull()]\n",
    "    print(data.shape)\n",
    "    num_cluster = len(data[\"cluster\"].unique())\n",
    "    all_clusters = sorted(data[\"cluster\"].unique())\n",
    "\n",
    "    for i in all_clusters:\n",
    "        for j in all_clusters:\n",
    "            if j > i:\n",
    "                i, j = int(i), int(j)\n",
    "                cluster_pw = data[(data[\"cluster\"] == i) | (data[\"cluster\"] == j)]\n",
    "                fstat_name = \"{}_{}_KCC_{}_F_stat_cluster_{}vs{}.csv\".format(\n",
    "                    method, view, KCC_space, i, j\n",
    "                )\n",
    "                F_stat = pd.read_csv(\n",
    "                    \"{}/{}\".format(f_stat_path, fstat_name), index_col=0\n",
    "                )\n",
    "\n",
    "                fisher_name = \"{}_{}_KCC_{}_FisherExact_cluster_{}vs{}.csv\".format(\n",
    "                    method, view, KCC_space, i, j\n",
    "                )\n",
    "                Fisher = pd.read_csv(\n",
    "                    \"{}/{}\".format(f_stat_path, fisher_name), index_col=0\n",
    "                )\n",
    "                Fisher.columns = [\"p-value\"]\n",
    "                Fisher[\"F_stat\"] = np.nan\n",
    "\n",
    "                F_stat = F_stat[F_stat[\"p-value\"] < 0.05]\n",
    "                Fisher = Fisher[Fisher[\"p-value\"] < 0.05 /total_vars]\n",
    "                stats = pd.concat([F_stat, Fisher])\n",
    "                stats = (\n",
    "                    stats.sort_values(\"p-value\")\n",
    "                    .sort_values(\"F_stat\", ascending=False)\n",
    "                    .iloc[:10]\n",
    "                )\n",
    "                fig_name = \"{}_{}_KCC_{}_TopFeatures_cluster_{}vs{}\".format(\n",
    "                    method, view, KCC_space, i, j\n",
    "                )\n",
    "                plot_top(\n",
    "                    cluster_pw, stats.index.tolist(), top_feature_plot_path, fig_name\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8cc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
